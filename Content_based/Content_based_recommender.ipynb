{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Content based recommender system\n",
    "\n",
    "Content based systems use meta data such as genre, producer, actor, musician to recommend items say movies or music. Content based systems are based on the idea that if you liked a certain item you are most likely to like something that is similar to it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from pprint import pprint\n",
    "from scipy.stats import pearsonr as pearsons_correlation\n",
    "import pyLDAvis.gensim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import warnings\n",
    "import os.path\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='bs4')\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()    \n",
    "stemma = PorterStemmer()\n",
    "moviesName = [] #To be returned.\n",
    "moviesId = [] #To be returned.\n",
    "numMovies = 0 #To be returned.\n",
    "moviesIndexMapping = {}\n",
    "moviesCorpus = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and get the movies dataset.\n",
    "dataextract = pd.read_csv('../data/movies.csv')\n",
    "numMovies = len(dataextract)\n",
    "col1 = dataextract['movieId']\n",
    "col2 = dataextract['title']\n",
    "col3 = dataextract['genres']\n",
    "for i in range(numMovies):\n",
    "    moviesName.append(col2[i])\n",
    "    moviesId.append(col1[i])\n",
    "    doc = []\n",
    "    wordsList = col3[i].split('|')\n",
    "    for j in range(len(wordsList)):\n",
    "        word = wordsList[j]\n",
    "        word = word.lower()\n",
    "        word = stemma.stem(lemma.lemmatize(word))\n",
    "        if word in list(stopwords.words('english')):\n",
    "            continue;\n",
    "        else:\n",
    "            doc.append(word)\n",
    "    moviesCorpus.append(doc)\n",
    "    moviesIndexMapping[col1[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load and get the tags dataset.\n",
    "dataextract = pd.read_csv('../data/tags.csv')\n",
    "col1 = dataextract['movieId']\n",
    "col2 = dataextract['tag']\n",
    "for word in dataextract:\n",
    "    word = word.lower()\n",
    "    word = stemma.stem(lemma.lemmatize(word))\n",
    "    if word in list(stopwords.words('english')):\n",
    "            continue;\n",
    "    else:\n",
    "        j = moviesIndexMapping[col1[i]]\n",
    "        moviesCorpus[j].append(word)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Build the recommender\n",
    "\n",
    "This content-based recommender is based on the matrix factorization by using the LDA topic modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of words. Create a dictionary containing the word and words unique ID.\n",
    "dictionary = gensim.corpora.Dictionary(moviesCorpus)\n",
    "dictionary.filter_extremes(no_below=5) #Alter according to dataset.\n",
    "\n",
    "\n",
    "#For each document a list of tuples is created reporting the words(ID) in filtered dictionary and frequency of those words.\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in moviesCorpus]\n",
    "\n",
    "\n",
    "#Get max coherence score to get perfect lda model. Get min perplexity to get perfect lda model.\n",
    "maxCoherence = 0\n",
    "numTopics = 0\n",
    "minPerplexity = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic:  5  Coherence Score:  0.48425831688005694  Perplexity:  -2.7489907696893474\n",
      "Topic:  6  Coherence Score:  0.48425831688005694  Perplexity:  -2.796620909284142\n",
      "Topic:  7  Coherence Score:  0.4842583168800569  Perplexity:  -2.828833152309498\n",
      "Topic:  8  Coherence Score:  0.4842583168800568  Perplexity:  -2.835953084298595\n",
      "Topic:  9  Coherence Score:  0.48425831688005694  Perplexity:  -2.861497600610528\n",
      "Topic:  10  Coherence Score:  0.4842583168800568  Perplexity:  -2.952712984204523\n",
      "Topic:  11  Coherence Score:  0.48425831688005694  Perplexity:  -2.94858947465324\n",
      "Topic:  12  Coherence Score:  0.48425831688005694  Perplexity:  -2.988007333009216\n",
      "Topic:  13  Coherence Score:  0.48425831688005694  Perplexity:  -3.024767358037005\n",
      "Topic:  14  Coherence Score:  0.4842583168800569  Perplexity:  -3.0402816760116544\n",
      "Topic:  15  Coherence Score:  0.4842583168800569  Perplexity:  -3.0488601101197097\n",
      "Topic:  16  Coherence Score:  0.4842583168800568  Perplexity:  -3.0946468391213324\n",
      "Topic:  17  Coherence Score:  0.4842583168800568  Perplexity:  -3.163113002761001\n",
      "Topic:  18  Coherence Score:  0.4842583168800568  Perplexity:  -3.1460723720615946\n",
      "Topic:  19  Coherence Score:  0.4842583168800568  Perplexity:  -3.148504532002451\n",
      "Topic:  20  Coherence Score:  0.4842583168800568  Perplexity:  -3.2615873769015766\n"
     ]
    }
   ],
   "source": [
    "# Runs the LDA model\n",
    "# It is first going to check whether the best model has been created already, in which case it loads it. Otherwise it will create it\n",
    "if os.path.exists('lda_model.pkl'):\n",
    "      ldamodel = pickle.load(open(ldafolder+'/%s_groups/lda_model_%semails.pkl'%(num_groups,n_sample), 'rb'))  \n",
    "else:\n",
    "    for t in range(5, 21):\n",
    "        lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=t, id2word=dictionary, passes=20, workers=3)\n",
    "        coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, texts=moviesCorpus, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherence_model_lda.get_coherence()\n",
    "        perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "        if coherence_score > maxCoherence:\n",
    "            maxCoherence = coherence_score\n",
    "            numTopics = t\n",
    "            minPerplexity = perplexity\n",
    "        print('Topic: ',t,' Coherence Score: ',coherence_score,' Perplexity: ', perplexity)\n",
    "        \n",
    "    #Train bow_corpus.\n",
    "    lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=numTopics, id2word=dictionary, passes=20, workers=3)\n",
    "    \n",
    "    # Save model\n",
    "    ldafile = open('lda_model.pkl','wb')\n",
    "    pickle.dump(lda_model,ldafile)\n",
    "    ldafile.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.859*\"comedi\" + 0.140*\"romanc\" + 0.000*\"action\" + 0.000*\"sci-fi\" + '\n",
      "  '0.000*\"crime\" + 0.000*\"horror\" + 0.000*\"drama\" + 0.000*\"child\" + '\n",
      "  '0.000*\"western\" + 0.000*\"anim\"'),\n",
      " (1,\n",
      "  '0.271*\"thriller\" + 0.191*\"crime\" + 0.173*\"horror\" + 0.129*\"drama\" + '\n",
      "  '0.099*\"mysteri\" + 0.092*\"action\" + 0.045*\"sci-fi\" + 0.000*\"film-noir\" + '\n",
      "  '0.000*\"romanc\" + 0.000*\"imax\"'),\n",
      " (2,\n",
      "  '0.427*\"fantasi\" + 0.313*\"music\" + 0.129*\"drama\" + 0.098*\"film-noir\" + '\n",
      "  '0.028*\"mysteri\" + 0.004*\"comedi\" + 0.000*\"crime\" + 0.000*\"romanc\" + '\n",
      "  '0.000*\"child\" + 0.000*\"anim\"'),\n",
      " (3,\n",
      "  '0.709*\"drama\" + 0.171*\"romanc\" + 0.076*\"war\" + 0.044*\"western\" + '\n",
      "  '0.000*\"action\" + 0.000*\"adventur\" + 0.000*\"comedi\" + 0.000*\"sci-fi\" + '\n",
      "  '0.000*\"crime\" + 0.000*\"mysteri\"'),\n",
      " (4,\n",
      "  '0.242*\"documentari\" + 0.199*\"adventur\" + 0.177*\"action\" + 0.107*\"child\" + '\n",
      "  '0.107*\"anim\" + 0.088*\"sci-fi\" + 0.049*\"(no genres listed)\" + 0.016*\"imax\" + '\n",
      "  '0.015*\"fantasi\" + 0.000*\"western\"')]\n"
     ]
    }
   ],
   "source": [
    "#For each topic, print top 10 significant terms.\n",
    "pprint(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/santi/anaconda3/lib/python3.6/site-packages/pyLDAvis/_prepare.py:257: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  return pd.concat([default_term_info] + list(topic_dfs))\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'lda_figure.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docTermMatrix = np.zeros((numMovies, numTopics)) #To be returned.\n",
    "\n",
    "\n",
    "#Fill document term matrix\n",
    "for i in range(len(moviesCorpus)):\n",
    "    doc = moviesCorpus[i]\n",
    "    bow_vector = dictionary.doc2bow(doc)\n",
    "    vec = lda_model[bow_vector]\n",
    "    #print(doc)\n",
    "    #print(vec)\n",
    "    for j in range(len(vec)):\n",
    "        t = vec[j]\n",
    "        docTermMatrix[i,j] = t[1];\n",
    "\n",
    "\n",
    "#It can be seen from docTermmatrix that each movie belongs solely to only one topic, and very few percent of it belongs to other topics.\n",
    "\n",
    "\n",
    "docMostRelevantTopic = np.argmax(docTermMatrix, axis=1) #ith index stores the index of the most relevant topic in ith document.\n",
    "\n",
    "\n",
    "numUsers = 0 #To be returned.\n",
    "usersIndexMapping = {} #To be returned.\n",
    "\n",
    "\n",
    "#Count number of users.\n",
    "dataextract = pd.read_csv('../data/ratings.csv')\n",
    "col1 = dataextract['userId']\n",
    "for i in range(len(dataextract)):\n",
    "    if col1[i] not in usersIndexMapping:\n",
    "        usersIndexMapping[col1[i]] = numUsers\n",
    "        numUsers += 1\n",
    "\n",
    "\n",
    "userTermMatrix = np.zeros((numUsers, numTopics)) #To be returned.\n",
    "userRateFreq = np.zeros((numUsers, numTopics))\n",
    "\n",
    "\n",
    "#Load and get dataset.\n",
    "col2 = dataextract['movieId']\n",
    "col3 = dataextract['rating']\n",
    "for i in range(len(dataextract)):\n",
    "    j = usersIndexMapping[col1[i]]\n",
    "    k = moviesIndexMapping[col2[i]]\n",
    "    #As each movie solely belongs to only one topic, taking only contribution of that main topic as other topics will only lead to error.\n",
    "    docMostRelevantTopicIndex = docMostRelevantTopic[k]\n",
    "    #Here we are not multiplying with rating as, in case of euclid similarity, multiplying by 5(rating) will take user point away from relevant topic(movie point) more that multiplying by 3(rating). So euclid distance will favour movie with rating 3 that movie with rating 5 as according to euclid similarity, less the distance more is similarity.\n",
    "    if col3[i]>=3:\n",
    "        userTermMatrix[j,docMostRelevantTopicIndex] += docTermMatrix[k,docMostRelevantTopicIndex]\n",
    "    else:\n",
    "        userTermMatrix[j,docMostRelevantTopicIndex] -= docTermMatrix[k,docMostRelevantTopicIndex]\n",
    "    userRateFreq[j,docMostRelevantTopicIndex] += 1\n",
    "userRateFreq[userRateFreq == 0] = 1\n",
    "for i in range(numUsers):\n",
    "    userTermMatrix[i] /= userRateFreq[i]\n",
    "\n",
    "\n",
    "#View userTermMatrix and docTermMatrix\n",
    "file1 = open(\"test1.txt\", \"w\")\n",
    "for i in range(numUsers):\n",
    "    file1.write(str(userTermMatrix[i]))\n",
    "    file1.write(\"\\n\")\n",
    "file1.close()\n",
    "file2 = open(\"test2.txt\", \"w\")\n",
    "for i in range(numMovies):\n",
    "    file2.write(str(docTermMatrix[i]))\n",
    "    file2.write(\"\\n\")\n",
    "file2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check and get accurracy. \n",
    "file = open(\"test3.txt\", \"w\")\n",
    "for i,elem in enumerate(dataextract):\n",
    "    uid = col1[i]\n",
    "    mid = col2[i]\n",
    "    rval = col3[i]\n",
    "    j = usersIndexMapping[uid]\n",
    "    uservec = userTermMatrix[j]\n",
    "    k = moviesIndexMapping[mid]\n",
    "    docvec = docTermMatrix[k]\n",
    "    docMostRelevantTopicIndex = docMostRelevantTopic[k]\n",
    "    #coeff, pval = pearsons_correlation(docvec, uservec) #Pearson's correlation similarity\n",
    "    #coeff = np.linalg.norm(docvec-uservec) #Euclidean distance similarity\n",
    "    #As each movie solely belongs to only one topic, so comparing only that main topic with user matrix.\n",
    "    coeff = abs(docvec[docMostRelevantTopicIndex]-uservec[docMostRelevantTopicIndex]) #Euclidean distance between only relevant topic.\n",
    "    string = str(uid)+\"\\t\"+str(mid)+\"\\t\"+str(rval)+\"\\t\"+str(coeff)+\"\\n\"\n",
    "    file.write(string)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(x):\n",
    "    return x[1]\n",
    "def run():\n",
    "    uid = int(input(\"Enter User Id: \"))\n",
    "    if uid not in usersIndexMapping.keys():\n",
    "        print(\"User Id not in record.\")\n",
    "        return\n",
    "    i = usersIndexMapping[uid]\n",
    "    uservec = userTermMatrix[i]\n",
    "    #print(uservec)\n",
    "    recFactor = []\n",
    "    for i in range(numMovies):\n",
    "        docvec = docTermMatrix[i]\n",
    "        docMostRelevantTopicIndex = docMostRelevantTopic[i]\n",
    "        #coeff, pval = pearsons_correlation(docvec, uservec) #Pearson's correlation similarity\n",
    "        #coeff = np.linalg.norm(docvec-uservec) #Euclidean distance similarity\n",
    "        #As each movie solely belongs to only one topic, so comparing only that main topic with user matrix.\n",
    "        coeff = abs(docvec[docMostRelevantTopicIndex]-uservec[docMostRelevantTopicIndex]) #Euclidean distance between only relevant topic.\n",
    "        #print(str(moviesName[i])+\" \"+str(moviesId[i]))\n",
    "        recFactor.append(tuple((i, coeff)))\n",
    "    recFactor = sorted(recFactor, key=operator.itemgetter(1), reverse=True)\n",
    "    numRec = 10\n",
    "    recommend = []\n",
    "    for j in range(numRec):\n",
    "        i = recFactor[j][0]\n",
    "        recommend.append(tuple((moviesName[i], moviesId[i])))\n",
    "    print(recommend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
